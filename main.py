from fastapi import FastAPI, Request
from typing import Dict, Any
import os
import requests
from dotenv import load_dotenv

from langchain.chat_models import ChatOpenAI
from agent_components import initialize_agent_components
from workflow import run_workflow

load_dotenv()
BACKEND_URL = os.getenv("BACKEND_URL", "http://localhost:8080")

app = FastAPI()

llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.5)
agent_components = initialize_agent_components(llm)

@app.post("/routine/alarm")
async def trigger_routine_alarm(req: Request):
    alert_data: Dict[str, Any] = await req.json()
    print(f"ë£¨í‹´ ì•Œë¦¼ ìˆ˜ì‹ : {alert_data}")

    # LangGraph ì‹¤í–‰
    final_answer = run_workflow(
        input_query="ë£¨í‹´ ì•Œë¦¼ ìš”ì²­ì…ë‹ˆë‹¤.",
        llm=llm,
        fall_alert=False,
        routine_alert_data=alert_data,
        agent_components=agent_components
    )

    return {
        "status": "success",
        "type": "routine_alert",
        "message": final_answer
    }



async def fall_detected():
    print("ğŸš¨ ë‚™ìƒ ê°ì§€ ì•Œë¦¼ ìˆ˜ì‹ ë¨")

    final_answer = run_workflow(
        input_query="ë‚™ìƒì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.",
        llm=llm,
        fall_alert=True,
        agent_components=agent_components
    )

    return {
        "status": "success",
        "type": "fall_alert",
        "message": final_answer
    }

@app.post("/user-request")
async def user_request(req: Request):
    data: Dict[str, Any] = await req.json()
    user_input = data.get("input", "")

    print(f"ğŸ‘¤ ìœ ì € ì…ë ¥ ìˆ˜ì‹ : {user_input}")

    final_answer = run_workflow(
        input_query=user_input,
        llm=llm,
        fall_alert=False,
        agent_components=agent_components
    )

    return {
        "status": "success",
        "type": "user_request",
        "message": final_answer
    }
